# 02注意力模块

申明：本教程的所有内容(文字，图片，代码等)可以用于非盈利目的个人使用和分享。但如果用于盈利目的，包括但不限于卖课，公众号，视频号等需要经由作者的批准。谢谢理解。

[\[主目录链接\]](https://github.com/KaihuaTang/All-you-need-to-know-about-LLM)

**如果您认为本教程对您的学业或事业有所帮助，愿意给我打赏以赞助我后续的教程和开源项目，欢迎通过下列链接给予赞助。** 

[\[赞助\]](https://kaihuatang.github.io/donate.html)      [(赞助者列表)](https://kaihuatang.github.io/supporters.html)

## 前言

## 一. 注意力原理

## 二. 自注意力结构

## 三. 注意力优化

## 四. 大语言模型中的注意力

### 1. Qwen2的注意力代码详解

### 2. LLaMA3的注意力代码详解

## 五. 扩展知识

## 引用链接

```
@misc{tang2025all,
title = {All you need to know about LLM: a LLM tutorial},
author = {Tang, Kaihua},
year = {2025},
note = {\url{https://github.com/KaihuaTang/All-you-need-to-know-about-LLM}},
}
```
